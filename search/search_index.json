{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"McCl8s","text":"<p>McCl8s is a tongue-in-cheek portmanteau of my last name and Kubernetes, or K8s. It is not a custom implementation of K8s, but a name for the collection of applications and resources deployed using MicroK8s as a platform for experimentation and learning.</p>"},{"location":"#sections","title":"Sections","text":"<ul> <li>Learning topics: Outlines the tools and concepts   targeted for learning as part of building McCl8s.</li> <li>Setup: Covers configuration and installation of   foundation hardware and software used to build McCl8s.</li> <li>Cluster setup with Ansible: Covers automation   of configuration and software installs for Raspberry Pi cluster   workers using Ansible.</li> <li>Managing a K8s cluster: Provides an overview   of options for managing resources in a Kubernetes cluster.</li> <li>First deployments: Serves as a guide for   creating and modifying our first Kubernetes resources.</li> <li>Kubeflow: Introduces Kubeflow and provides guidance   for deployment.</li> <li>Vanilla Kubernetes: Discusses the work involved to   start up a Kubernetes cluster using <code>kubeadm</code> rather than   Microk8s.</li> </ul>"},{"location":"ansible_cluster/","title":"Cluster setup with Ansible","text":"<p>For this setup, four Raspberry Pi nodes are available on the network. We will configure one as both a network DNS server and as a test node for applying Ansible playbooks. We will configure the remaining three nodes to join our MicroK8s cluster as workers nodes.</p>"},{"location":"ansible_cluster/#getting-started","title":"Getting started","text":"<p>Ansible documentation introduces three main components of an Ansible environment:</p> <ol> <li>A control node, where Ansible is installed.</li> <li>Managed nodes, remote hosts that Ansible controls.</li> <li>Inventory, a list of logically organized managed nodes.</li> </ol> <p>In this parlance, the WSL 2 machine will behave as the control node, and the four Raspberry Pi machines as managed nodes.</p>"},{"location":"ansible_cluster/#creating-the-inventory","title":"Creating the inventory","text":"<p>Ansible inventories defines the managed nodes to automate, with groups so you can run automation tasks on multiple hosts at the same time. With an inventory defined, you can use patterns to select the hosts or groups for Ansible to run against.</p> <p>For example, with a cluster of 4 Raspberry Pi systems with a default user <code>pi</code>, an inventory might look like this:</p> <pre><code>[raspberrypis]\n192.168.1.100\n192.168.1.101\n192.168.1.102\n192.168.1.103\n\n[raspberrypis:vars]\nansible_user=pi\n</code></pre> <p>The <code>ansible_user</code> ensures connections are established using the <code>pi</code> user if the user on the control node is another username.</p> <p>The default location for the inventory file is <code>/etc/ansible/hosts</code>. One can use another location and specify that file when running ansible commands (here, with <code>-i ~/.ansible/etc/hosts</code>):</p> <pre><code>ansible -i ~/.ansible/etc/hosts raspberrypis -m ping\n192.168.1.102 | SUCCESS =&gt; {\n\"ansible_facts\": {\n\"discovered_interpreter_python\": \"/usr/bin/python3\"\n},\n    \"changed\": false,\n    \"ping\": \"pong\"\n}\n192.168.1.101 | SUCCESS =&gt; {\n\"ansible_facts\": {\n\"discovered_interpreter_python\": \"/usr/bin/python3\"\n},\n    \"changed\": false,\n    \"ping\": \"pong\"\n}\n192.168.1.100 | SUCCESS =&gt; {\n\"ansible_facts\": {\n\"discovered_interpreter_python\": \"/usr/bin/python3\"\n},\n    \"changed\": false,\n    \"ping\": \"pong\"\n}\n192.168.1.103 | SUCCESS =&gt; {\n\"ansible_facts\": {\n\"discovered_interpreter_python\": \"/usr/bin/python3\"\n},\n    \"changed\": false,\n    \"ping\": \"pong\"\n}\n</code></pre> <p>A richer format for an inventory file is YAML. Here, we modify the previous example to provide named hosts for each IP:</p> <pre><code>raspberrypis:\nhosts:\nraspberrypi-0:\nansible_host: 192.168.1.100\nraspberrypi-1:\nansible_host: 192.168.1.101\nraspberrypi-2:\nansible_host: 192.168.1.102\nraspberrypi-3:\nansible_host: 192.168.1.103\nvars:\nansible_user: pi\n</code></pre> <p>The inventory we'll use throughout this documentation looks like:</p> <pre><code>pis:\nchildren:\ncanary:\nhosts:\nraspberrypi-0:\nansible_host: 192.168.1.100\nraspi3s:\nhosts:\nraspberrypi-0:\nansible_host: 192.168.1.100\nraspberrypi-1:\nansible_host: 192.168.1.101\nraspberrypi-2:\nansible_host: 192.168.1.102\nraspberrypi-3:\nansible_host: 192.168.1.103\nraspi4s:\nhosts:\nvatomouro-0:\nansible_host: 192.168.1.110\nvatomouro-1:\nansible_host: 192.168.1.111\nvatomouro-2:\nansible_host: 192.168.1.112\nvatomouro-3:\nansible_host: 192.168.1.113\nraspis:\nchildren:\nraspi3s:\nraspi4s:\nrockpis:\nhosts:\nvrachos-0:\nansible_host: 192.168.1.120\nvrachos-1:\nansible_host: 192.168.1.121\nvrachos-2:\nansible_host: 192.168.1.122\nvrachos-3:\nansible_host: 192.168.1.123\nvrachos-4:\nansible_host: 192.168.1.124\nvrachos-5:\nansible_host: 192.168.1.125\nvars:\nansible_user: rock\nk8scontrol:\nhosts:\nvatomouro-0:\nansible_host: 192.168.1.110\nvatomouro-1:\nansible_host: 192.168.1.111\nvatomouro-2:\nansible_host: 192.168.1.112\nk8sworkers:\nhosts:\nvatomouro-3:\nansible_host: 192.168.1.113\nvrachos-0:\nansible_host: 192.168.1.120\nvrachos-1:\nansible_host: 192.168.1.121\nvrachos-2:\nansible_host: 192.168.1.122\nk8sstorage:\nhosts:\nvrachos-3:\nansible_host: 192.168.1.123\nvrachos-4:\nansible_host: 192.168.1.124\nvrachos-5:\nansible_host: 192.168.1.125\nk8s:\nchildren:\nk8scontrol:\nk8sworkers:\nk8sstorage:\nvars:\nansible_user: pi\ncontrol_plane_endpoint: k8s.clunacy.dev\napiserver_vip: 192.168.1.150\npod_network_cidr: 10.90.0.0/16\nip_address_pool_addresses: 192.168.1.192/27\n</code></pre> <p>This inventory defines a <code>raspberrypis</code> group that contains to child groups: a <code>canary</code> test group, and a <code>k8s</code> group. All hosts are accessed as user <code>pi</code>, so we define <code>ansible_user</code> accordingly.</p>"},{"location":"ansible_cluster/#creating-roles","title":"Creating roles","text":"<p>Echoing the Roles documentation for Ansible:</p> <p>Roles let you automatically load related vars, files, tasks, handlers, and other Ansible artifacts based on a known file structure. After you group your content in roles, you can easily reuse them and share them with other users.</p> <p>There are three roles needed to configure the Pis:</p> <ol> <li>A <code>common</code> role, for tasks shared by all nodes. Every node, for example,    should be updated and upgraded. And every node should have <code>snap</code> and    <code>microk8s</code> installed.</li> <li>A <code>dnsservers</code> role, unique to the test node that we will use as our    local network DNS server.</li> <li>A <code>workers</code> role, unique to the <code>k8s</code> group, the nodes that will join    the <code>microk8s</code> cluster as workers.</li> </ol> <p>NOTE: The WSL 2 machine is behaving as both the MicroK8s cluster control plane and the Ansible control node.</p> <p>The details of the <code>common</code> and <code>dnsservers</code> roles are not covered here, but the implementations for each can be viewed in the source code. The <code>tasks/main.yml</code> for the <code>workers</code> is shown below:</p> <pre><code>--8&lt;-- ansible/roles/workers/tasks/main.yml\n</code></pre> <p>Worth highlighting here are the presence of variables that Ansible can populate either from variables files or the command line at runtime. We will make use of the <code>microk8s_instance</code> variable when running an associated playbook.</p>"},{"location":"ansible_cluster/#setting-up-playbooks","title":"Setting up playbooks","text":"<p>With our three roles defined, there are three associated playbooks to create:</p> <ol> <li><code>site.yml</code> to update all nodes and install <code>microk8s</code>,</li> <li><code>dns.yml</code> to create a DNS server (and to update all other nodes to use    that server), and</li> <li><code>k8sworkers.yml</code> to join <code>k8s</code> nodes to the MicroK8s cluster.</li> </ol> <p>In each, we can refer to associated roles to run their tasks. For example, the <code>k8sworkers.yml</code> playbook looks like this:</p> <pre><code>\n</code></pre> <p>The remaining two playbooks source code. In our <code>k8sworkers.yml</code> playbook, we can see by default that <code>k8s</code> hosts are targeted from our inventory, but that <code>variable_host</code> can be used to override that setting at runtime. This is useful for testing configuration changes on our <code>canary</code> host before rolling out to <code>k8s</code> hosts.</p>"},{"location":"ansible_cluster/#running-playbooks","title":"Running playbooks","text":"<p>Playbooks are run using the <code>ansible-playbook</code> CLI:</p> <pre><code>ansible-playbook -h\n</code></pre> <p>For our playbooks, we'll run them with one other argument to indicate the inventory file to use. For example, running the <code>site.yml</code> playbook from our <code>ansible/</code> repository directory looks like this:</p> <pre><code>cd ansible/\nansible-playbook -i inventory.yml site.yml\n</code></pre> <p>The more interesting playbook is our <code>k8sworkers.yml</code> playbook. To add nodes to MicroK8s, the follow command must first be run from cluster master node:</p> <pre><code>microk8s add-node\n</code></pre> <p>It outputs instructions like the following, which include URLs with unique tokens and associated expiry times:</p> <pre><code>From the node you wish to join to this cluster, run the following:\nmicrok8s join 192.168.1.230:25000/92b2db237428470dc4fcfc4ebbd9dc81/2c0cb3284b05\n\nUse the '--worker' flag to join a node as a worker not running the control plane, eg:\nmicrok8s join 192.168.1.230:25000/92b2db237428470dc4fcfc4ebbd9dc81/2c0cb3284b05 --worker\n\nIf the node you are adding is not reachable through the default interface you can use one of the following:\nmicrok8s join 192.168.1.230:25000/92b2db237428470dc4fcfc4ebbd9dc81/2c0cb3284b05\nmicrok8s join 10.23.209.1:25000/92b2db237428470dc4fcfc4ebbd9dc81/2c0cb3284b05\nmicrok8s join 172.17.0.1:25000/92b2db237428470dc4fcfc4ebbd9dc81/2c0cb3284b05\n</code></pre> <p>To run our playbook, we first need to obtain a join URL from the MicroK8s cluster master, and then provide that URL to the task in <code>workers</code> role, awaiting it via the variable <code>microk8s_instance</code> (see Creating roles).</p> <p>Running that playbook might look like this:</p> <pre><code>ansible-playbook -i inventory.yml k8sworkers.yml \\\n--extra-vars \"microk8s_instance=192.168.1.230:25000/92b2db237428470dc4fcfc4ebbd9dc81/2c0cb3284b05\"\n</code></pre> <p>Here's a handy one-liner to both extend the token expiry time and capture the URL:</p> <pre><code>ansible-playbook -i inventory.yml k8sworkers.yml \\\n--extra-vars \"microk8s_instance=$(microk8s add-node --token-ttl 3600 | grep microk8s | head -1 | cut -d' ' -f3)\"\n</code></pre>"},{"location":"first_deployments/","title":"First deployments","text":"<p>With setup complete, we can begin deployments.</p>"},{"location":"first_deployments/#enabling-microk8s-addons","title":"Enabling MicroK8s addons","text":"<p>To start, let's enable the standard Kubernetes dashboard:</p> <pre><code>microk8s enable dashboard\n</code></pre> <p>Then, enable addons that will be required to install Kubeflow. In the below example, we can also set an IP range for <code>metallb</code> to use for external IPs:</p> <pre><code>microk8s enable dns hostpath-storage ingress metallb:192.168.1.192/27\n</code></pre>"},{"location":"first_deployments/#patching-the-ingress-controller","title":"Patching the ingress controller","text":"<p>The <code>ingress</code> addon runs <code>ingress-nginx</code>, a Kubernetes Ingress NGINX controller. We need to enable SSL passthrough to allow passthrough backends to Ingress objects. Do that by running the following:</p> <pre><code>k patch daemonset -n ingress nginx-ingress-microk8s-controller \\\n--type=json \\\n-p='[{\"op\": \"add\", \"path\": \"/spec/template/spec/containers/0/args/-\", \"value\": \"--enable-ssl-passthrough\"}]'\n</code></pre>"},{"location":"first_deployments/#adding-an-ingress-service","title":"Adding an ingress Service","text":"<p>With <code>metallb</code> and <code>ingress</code> addons both active, we can follow MetalLB addon guidance to set up an ingress Service. That can be done via <code>kubectl</code>, but we'll use Kustomize. We define our ingress Service resource, a LoadBalancer, in <code>k8s/nginx-ingress/base</code>:</p> <pre><code>\n</code></pre> <p>Using our <code>prod</code> overlay, apply the service by running:</p> <pre><code>k apply -k k8s/nginx-ingress/overlays/prod\n</code></pre>"},{"location":"first_deployments/#exposing-the-kubernetes-dashboard","title":"Exposing the Kubernetes Dashboard","text":"<p>With a LoadBalancer running, we can expose the Kubernetes Dashboard, making it reachable via an external IP. As with the ingress Service, we have Kustomize resources prepared for the dashboard. Apply those kustomizations by running:</p> <pre><code>k apply -k k8s/kubernetes-dashboard/overlays/prod\n</code></pre> <p>Beyond the scope of the current documentation:</p> <ul> <li>The Ansible dns.yml playbook is also configured to add the hostname of the   Ingress, &gt; <code>kubernetes-dashboard.k8s.local</code>, to the DNS server.</li> <li>The default certificates of the dashboard are self-signed and will not be   trusted by browsers. However, setting up a   private CA with cfssl   and updating the <code>kubernetes-dashboard-certs</code> secret is an option to   serve a trusted Dashboard.</li> </ul> <p>Provided a <code>./certs</code> directory with a <code>tls.crt</code> and <code>tls.key</code>, run the following to get replace the self-signed default certificates with one of your own:</p> <pre><code># Needs to hold CA-signed tls.crt and tls.key\nk create secret generic -n kube-system kubernetes-dashboard-certs --from-file=./certs\n# Update deployment per\n# https://github.com/kubernetes/dashboard/blob/master/docs/user/installation.md#recommended-setup\nk patch deployments.apps -n kube-system kubernetes-dashboard \\\n--type=json \\\n-p='[{\"op\": \"add\", \"path\": \"/spec/template/spec/containers/0/args/-\", \"value\": \"--tls-cert-file=/tls.crt\"}]'\nk patch deployments.apps -n kube-system kubernetes-dashboard \\\n--type=json \\\n-p='[{\"op\": \"add\", \"path\": \"/spec/template/spec/containers/0/args/-\", \"value\": \"--tls-key-file=/tls.key\"}]'\n</code></pre> <p>When visiting the Kubernetes Dashboard, a token will be required for access. Use the <code>admin-user</code> to generate one by running:</p> <pre><code>k create -n kube-system token admin-user\n</code></pre> <p>Enter the displayed token where prompted in the Dashboad.</p>"},{"location":"first_deployments/#tekton","title":"Tekton","text":"<p>Per Tekton documentation,</p> <p>Tekton is a cloud-native solution for building CI/CD systems. It consists of Tekton Pipelines, which provides the building blocks, and of supporting components, such as Tekton CLI and Tekton Catalog, that make Tekton a complete ecosystem.</p>"},{"location":"first_deployments/#deploying-tekton-resources","title":"Deploying Tekton resources","text":"<p>To deploy Tekton Pipelines and Tekton Triggers, apply the following kustomizations:</p> <pre><code>k apply -k k8s/tekton-pipelines/overlays/prod\nk apply -k k8s/tekton-triggers/overlays/prod\n</code></pre>"},{"location":"first_deployments/#deploying-the-dashboard","title":"Deploying the Dashboard","text":"<p>The kustomizations for Tekton Dashboard refer to a <code>tekton-pipelines</code> namespace secret, <code>tekton-dashboard-tls</code>. We'll need to make that first. Creating a TLS certificate/key pair is beyond the scope of the current documentation, however you may follow guidance to create private CA with cfssl here. The documentation example below assumes a <code>certificates/</code> directory with certificate and key files for the dashboard.</p> <p>To create the <code>tekton-dashboard-tls</code> certificate, refer to our certificate and key by running:</p> <pre><code>k create -n tekton-pipelines secret tls tekton-dashboard-tls \\\n--cert=certificates/tekton-dashboard-fullchain.pem \\\n--key=certificates/tekton-dashboard-key.pem\n</code></pre> <p>With the secret available, apply the dashboard kustomizations:</p> <pre><code>k apply -k k8s/tekton-dashboard/overlays/prod\n</code></pre>"},{"location":"first_deployments/#setting-up-tekton-cli","title":"Setting up Tekton CLI","text":"<p>The <code>tkn</code> CLI needs to be able to read a <code>kubeconfig</code> file. When using MicroK8s, the default file is neither <code>~/.kube/config</code> or is it given by environment variable <code>KUBECONFIG</code>. By running:</p> <pre><code>k get pods -v=6\n</code></pre> <p>You will see output like the following:</p> <pre><code>I1204 07:52:59.522754  200900 loader.go:374] Config loaded from file:  /var/snap/microk8s/4221/credentials/client.config\n</code></pre> <p>That instance identifier <code>4221</code> in the example above is symbolically linked to <code>current</code>, so one solution for getting <code>tkn</code> to work is to use:</p> <pre><code>alias tkn=\"env KUBECONFIG=/var/snap/microk8s/current/credentials/client.config\"\n</code></pre>"},{"location":"k8s_management/","title":"Managing a MicroK8s cluster","text":"<p>The Kubernetes documentation includes a great conceptual explanation of Kubernetes Objects:</p> <p>Kubernetes objects are persistent entities in the Kubernetes system. Kubernetes uses these entities to represent the state of your cluster. Specifically, they can describe:</p> <ul> <li>What containerized applications are running (and on which nodes)</li> <li>The resources available to those applications</li> <li>The policies around how those applications behave, such as restart   policies, upgrades, and fault-tolerance</li> </ul> <p>A Kubernetes object is a \"record of intent\"--once you create the object, the Kubernetes system will constantly work to ensure that object exists. By creating an object, you're effectively telling the Kubernetes system what you want your cluster's workload to look like; this is your cluster's desired state.</p> <p>We'll use a few different methods for working with Kubernetes Objects, including:</p> <ul> <li>MicroK8s addons</li> <li><code>kubectl</code>, the Kubernetes CLI</li> <li>Kustomize, the Kubernetes customization tool</li> <li>Juju, Charmed Operator framework</li> </ul>"},{"location":"k8s_management/#microk8s-addons","title":"MicroK8s addons","text":"<p>MicroK8s addons are extra services that can be enabled in MicroK8s, creating Objects, effectively functioning as a Kubernetes Operator. We will use addons to enable:</p> <ul> <li><code>dashboard</code>: The standard Kubernetes dashboard</li> <li><code>dns</code>: CoreDNS</li> <li><code>ingress</code>: A simple ingress controller for external access</li> <li><code>metallb</code>: The MetalLB LoadBalancer</li> <li><code>hostpath-storage</code>: A default storage class</li> </ul> <p>We can do that in one line (here providing <code>metallb</code> with an IP address range to use for LoadBalancers) like this:</p> <pre><code>microk8s enable dashboard dns ingress metallb:192.168.1.192/27 hostpath-storage\n</code></pre>"},{"location":"k8s_management/#kubectl","title":"<code>kubectl</code>","text":"<p><code>kubectl</code> is the CLI for communicating with the Kubernetes control plane, and is able to perform a large set of operations including <code>create</code>, <code>get</code>, <code>describe</code>, and <code>delete</code>.</p> <p>We will use <code>kubectl</code> primarily for four operations:</p> <ol> <li>To <code>patch</code> addons</li> <li>To <code>create</code> secrets</li> <li>To <code>create</code> tokens</li> <li>To run <code>kustomize</code> (see Kustomize below)</li> </ol> <p>An example of something to patch is the <code>ingress</code> controller, to enable SSL passthrough for services that need to handle TLS themselves.</p> <pre><code>k patch daemonset -n ingress nginx-ingress-microk8s-controller \\\n--type=json \\\n-p='[{\"op\": \"add\", \"path\": \"/spec/template/spec/containers/0/args/-\", \"value\": \"--enable-ssl-passthrough\"}]'\n</code></pre> <p>Secrets can be created of different kinds, including <code>tls</code> and Opaque (<code>generic</code>) secrets. For example, to create an Opaque secret containing TLS certificate and key files from a <code>./certs/</code> directory, you can run:</p> <pre><code>k create secret generic -n kube-system kubernetes-dashboard-certs --from-file=./certs\n</code></pre> <p>Tokens can be created to provide user credentials. For example, to generate a token for <code>admin-user</code> you can run:</p> <pre><code>k create token admin-user\n</code></pre> <p>NOTE: In the above commands, <code>microk8s kubectl</code> has been aliased to <code>k</code>.</p>"},{"location":"k8s_management/#kustomize","title":"Kustomize","text":"<p>Kustomize is a standalone tool for customizing Kubernetes objects using kustomization files. Through those files, Kustomize allows you to:</p> <ul> <li>generate resources from other sources</li> <li>set cross-cutting fields for resources</li> <li>compose and customize collections of resources</li> </ul> <p>The Kustomize GitHub page features a great explanation of usage. We will use <code>kustomize</code> to modify and create resources on the cluster, including:</p> <ul> <li>Creating a LoadBalancer Service for the cluster</li> <li>Creating Ingress resources for the <code>dashboard</code> addon</li> <li>Deploying Tekton Pipelines</li> </ul> <p>A simple example of a Kustomization with a production overlay is the LoadBalancer Service. We define that Service with the following resource definition, <code>k8s/nginx-ingress/base/ingress-service.yaml</code>:</p> <pre><code>\n</code></pre> <p>We declare that resource in our kustomization file, <code>k8s/nginx-ingress/base/kustomization.yaml</code>:</p> <pre><code># https://argo-cd.readthedocs.io/en/stable/operator-manual/installation/#kustomize\n# https://docs.nginx.com/nginx-ingress-controller/installation/installation-with-manifests/\napiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\n\nnamespace: ingress\nresources:\n# Deploy the ingress controller\n# https://raw.githubusercontent.com/nginxinc/kubernetes-ingress/v3.0.0/deployments/daemon-set/nginx-ingress.yaml\n- nginx-ingress.yaml\n# Get access to the ingress controller\n- https://raw.githubusercontent.com/nginxinc/kubernetes-ingress/v3.0.0/deployments/service/loadbalancer.yaml\n</code></pre> <p>Note both of these files live in a <code>base/</code> directory. In a neighboring <code>overlays/prod</code> directory, we create our trivial production overlay, <code>k8s/nginx-ingress/overlays/prod/kustomization.yaml</code>:</p> <pre><code>apiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\n\nnamespace: ingress\nresources:\n- ../../base\n</code></pre> <p>In this case, <code>prod</code> does not modify <code>base</code> at all, but it can and generally will, for example applying labels to indicate the configuration and to modify resources.</p> <p>With the kustomizations defined, we can apply our <code>prod</code> overlay to the cluster as follows:</p> <pre><code>k apply -k k8s/nginx-ingress/overlays/prod\n</code></pre> <p>The implementations of all kustomizations can be viewed in the source code.</p>"},{"location":"k8s_management/#juju","title":"Juju","text":"<p>Juju is an open source framework that uses Charmed Operators, or Charms, to deploy cloud infrastructure and operations.</p> <p>Our use case for using Juju is to install Kubeflow, a machine learning toolkit for Kubernetes. Specifically, we will use it to deploy Charmed Kubeflow's <code>kubeflow-lite</code> bundle.</p> <p>For installation steps, please refer to Charmed Kubeflow guidance.</p>"},{"location":"kubeflow/","title":"Kubeflow","text":"<p>Per the Kubeflow documentation,</p> <p>The Kubeflow project is dedicated to making deployments of machine learning (ML) workflows on Kubernetes simple, portable and scalable. Our goal is not to recreate other services, but to provide a straightforward way to deploy best-of-breed open-source systems for ML to diverse infrastructures. Anywhere you are running Kubernetes, you should be able to run Kubeflow.</p>"},{"location":"kubeflow/#installation","title":"Installation","text":"<p>NOTE: As of this writing, the installation guidance only works for a single-node MicroK8s cluster. Do not add the Pi nodes as workers. The first apparent issue: the <code>hostpath-storage</code> addon creates a PersistentVolume on the master node (<code>zephyrus</code>) but the PersistentVolumeClaim (PVC) created by Juju as part of installing Kubeflow maps to a Pi node, meaning the PVC will pend indefinitely.</p> <p>It is also possible that the RAM requirements of Kubeflow would exceed those available on the Pi nodes.</p> <p>As discussed in Managing a K8s cluster, we will install Charmed Kubeflow using <code>juju</code>. With <code>microk8s</code> and <code>juju</code> already installed, the steps are:</p> <ol> <li>Bootstrap Juju to MicroK8s, deploying a controller to MicroK8s' Kubernetes:</li> </ol> <pre><code>juju bootstrap microk8s\n</code></pre> <p>The contoller is Juju's agent, running on Kubernetes, which can be used to    deploy and control the components of Kubeflow. The controller works with    <code>models</code>, which map to namespaces in Kubernetes.</p> <ol> <li>Add a model! For Kubeflow, the model must be named <code>kubeflow</code>:</li> </ol> <pre><code>juju add-model kubeflow\n</code></pre> <ol> <li>Deploy a Kubeflow bundle. We will deploy a lighter option, <code>kubeflow-lite</code>:</li> </ol> <pre><code>juju deploy kubeflow-lite --trust\n</code></pre> <ol> <li>Wait for and monitor the deployment process. It can take tens of minutes:</li> </ol> <pre><code>watch -c juju status --color\n</code></pre>"},{"location":"kubeflow/#dashboard-access","title":"Dashboard access","text":"<p>With Kubeflow deployed, the next step is dashboard access. The dashboard is accessed through a central <code>istio-ingressgateway</code>. We first need to find the external IP assigned to its LoadBalancer. To do that, run the following:</p> <pre><code>k -n kubeflow get svc istio-ingressgateway-workload \\\n-o jsonpath='{.status.loadBalancer.ingress[0].ip}'\n</code></pre> <p>NOTE: Here, <code>k</code> is an alias for <code>microk8s kubectl</code>.</p> <p>For this documentation, we will presume the IP is <code>192.168.1.193</code>. Next, set up Dex authentication credentials:</p> <pre><code>juju config dex-auth static-username=admin\njuju config dex-auth static-password=password\n</code></pre> <p>NOTE: These are trivial credentials, feel free to choose a more secure password.</p> <p>Finally, access the dashboard with the IP captured above via browser, and log in using your chosen credentials.</p> <p>NOTE: As of this writing, only insecure HTTP access is working. Secure HTTP access is being refused. Documentation will be updated when this is resolved.</p>"},{"location":"kubeflow/#challenges","title":"Challenges","text":""},{"location":"kubeflow/#jupyterhub","title":"JupyterHub","text":"<p>With JupyterHub, there were a couple problems:</p> <ol> <li>Version assumptions in the Dashboard. The Kubeflow Dashboard Notebooks    links make assumptions about the API endpoints of the running JupyterLab    servers. Trying to run the latest Jupyter images (e.g.,    <code>tensorflow/tensorflow:latest-jupyter</code>) causes errors when trying to    connect to the running notebook.</li> <li>Inability to save notebooks. The notebooks ran fine when selecting    older images (such as those suggested in the images dropdown) but were    not saveable. JupyterLab would regularly display an error modal window    indicating that autosave could not be completed because the targeted    location on disk was read-only.</li> </ol>"},{"location":"kubeflow/#kubeflow-pipelines","title":"Kubeflow Pipelines","text":"<p>Kubeflow Pipelines worked fine when using the Kubeflow Pipelines compiler, as demonstrated in the Charmed Kubeflow example. However, attempts to use the <code>kfp.Client</code> class, ultimately met with SSL errors attempting to interact with the <code>ml-pipeline</code> service, raising errors like the following:</p> <pre><code>ssl.SSLError: [SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:1131)\n</code></pre>"},{"location":"kubeflow/#katib-and-tensorboard","title":"Katib and Tensorboard","text":"<p>The hyperparameter tuning and experiment visualization tools were out of scope of this exploration. Despite being visible on the Kubeflow Dashboard, the <code>kubeflow-lite</code> bundle does not include Katib or Tensorboard controllers.</p>"},{"location":"setup/","title":"Setup","text":"<p>McCl8s is a 4-node K8s cluster deployed on four nodes. A Windows 11 machine (with Ubuntu 22.04 via WSL 2) is used to run the control plane, and three Raspberry Pi 3 machines are used as worker nodes.</p> <p>In this section we'll cover setup, installation, and configuration work for:</p> <ol> <li>WSL-2</li> <li>Windows firewall</li> <li>Podman</li> <li>MicroK8s</li> <li>Ansible</li> </ol>"},{"location":"setup/#wsl-2","title":"WSL 2","text":"<p>NOTE: As of November 15, 2022, WSL 2 has reached 1.0.0, with Microsoft removing the preview label and making it generally available in the Microsoft Store. Instructions in this section assume the WSL 2 has been updated to 1.0.0.</p>"},{"location":"setup/#switching-networking-to-bridged-mode","title":"Switching networking to bridged mode","text":"<p>By default, WSL 2 runs in Network Access Translation (NAT) mode, meaning it has a virtualized ethernet adapter with its own unique IP address. This makes it challenging to access a WSL 2 distribution from another local area network (LAN) machine. To enable access, there are two options:</p> <ol> <li>To go through the same steps as one would for a regular virtual machine    with an internal virtual network. This approach for    accessing a WSL 2 distribution from a LAN    is documented by Microsoft.</li> <li>To configure Windows, Hyper-V, and WSL 2 to support and run in bridged mode,    allowing the WSL 2 distribution to acquire an IP address itself in the    LAN.</li> </ol>"},{"location":"setup/#option-1-an-example","title":"Option 1: an example","text":"<p>IP addresses for the Ubuntu WSL 2 distribution are captured into the <code>$hostnames</code> array. The primary IP address is then used as the connect address for a port proxy started by <code>netsh</code>:</p> <pre><code>&gt; $hostnames = @(&amp; wsl -d Ubuntu hostname -I) -split ' '\n&gt; netsh interface portproxy add v4tov4 `\n    listenport=25000 listenaddress=0.0.0.0 `\n    connectport=25000 connectaddress=$hostnames[0]\n</code></pre>"},{"location":"setup/#option-2-by-reference","title":"Option 2: by reference","text":"<p>Setting up WSL 2 to run in bridged mode involves:</p> <ul> <li>Ensuring minimum version requirements for WSL 2 itself</li> <li>Setting up a bridged virtual switch in Hyper-V</li> <li>Reconfiguring WSL 2</li> </ul> <p>This approach was discussed in a WSL GitHub feature request and more completely documented in this WSL 2 bridged mode networking guide. Rather than repeating the documentation here, please follow this excellent guide!</p> <p>Follow optional steps to enable <code>systemd</code>. It's a prerequisite for <code>snap</code>, which is used to install MicroK8s.</p>"},{"location":"setup/#why-option-2-is-necessary","title":"Why Option 2 is necessary","text":"<p>Option 1 is the simpler of the two, and for anonymous accessibility, both options work, making Option 1 an easy choice.</p> <p>However, for the K8s cluster use case, there are two problems:</p> <ol> <li>Inbound connections from a port proxy will not come from the LAN    IP address of the client, but the IP of the NAT router. MicroK8s will    refuse to add nodes whose self-reported machine names do not match (per    DNS lookup) the name associated with their IP.</li> <li>Because all inbound connections will appear to WSL 2 as having come from    the router IP, it is impossible to distinguish incoming MicroK8s join    requests and therefore impossible to add multiple workers from the LAN.</li> </ol> <p>Those complications disappear with a network bridge, making Option 2 the necessary choice for a WSL 2 + Raspberry Pi MicroK8s cluster.</p>"},{"location":"setup/#windows-firewall","title":"Windows firewall","text":"<p>The Windows (WSL 2) machine will serve as the control plane for the cluster, and must be reachable by worker nodes. To be reachable, the Windows Defender firewall must be configured with new rules. ICMP access (used to ping) and TCP access involve different rules. The following links provide guidance to create an inbound ICMP rule and tolle TCP access to the machine (including for specific ports or port ranges), follow the link to create an inbound port rule.</p>"},{"location":"setup/#podman","title":"Podman","text":"<p>Podman is a daemonless container engine for developing and managing OCI containers. Installing Podman is straightforward for most Linux OSes, with packages available in most official repositories. It can be installed in Ubuntu as follows:</p> <pre><code># Ubuntu 20.10 and newer\nsudo apt-get -y update\nsudo apt-get -y install podman\n</code></pre>"},{"location":"setup/#vs-code","title":"VS Code","text":"<p>As discussed in this GitHub  issue, Podman can be used in place of Docker in the <code>vscode-docker</code> extension using the following steps:</p> <ol> <li>Run: <code>systemctl --user enable --now podman.socket</code></li> <li>Set VS Code option <code>docker.host</code> to <code>unix:///run/user/1000/podman/podman.sock</code>    (this will persist once set)</li> </ol> <p>NOTE: docker.host was deprecated and removed as of Docker extension version 1.23. The new extension allows the user to specify environment variable/value pairs, but <code>DOCKER_HOST</code> does not work as of this writing. Follow this issue to track changes.</p>"},{"location":"setup/#microk8s","title":"MicroK8s","text":"<p>MicroK8s is a simple and fast way to get Kubernetes up and running, both for development and production use. For the McCl8s cluster, getting MicroK8s installed takes a few steps.</p>"},{"location":"setup/#enabling-systemd","title":"Enabling <code>systemd</code>","text":"<p>If <code>systemd</code> was not enabled as part of configuring WSL 2, read more about how to enable it in this Microsoft developer blog. It's a prerequisite for <code>snap</code>, which comes next.</p>"},{"location":"setup/#installing-snapd","title":"Installing <code>snapd</code>","text":"<p>Snaps are app packages for Linux that are cross-platform and dependency-free. <code>snapd</code> can be installed on Ubuntu (if not already installed) by running:</p> <pre><code>sudo apt update\nsudo apt install snapd\n</code></pre> <p>As discussed on Snap's installation page, you can optionally test <code>snap</code> after restarting by installing and running a hello-world snap:</p> <pre><code>sudo snap install hello-world\n</code></pre> <p><code>hello-world 6.4 from Canonical\u2713 installed</code></p> <pre><code>hello-world\n</code></pre> <p><code>Hello World!</code></p>"},{"location":"setup/#installing-microk8s","title":"Installing MicroK8s","text":"<p>MicroK8s can be installed as a <code>snap</code> package after Installing <code>snapd</code>:</p> <pre><code>sudo snap install microk8s --classic\n</code></pre> <p>Once MicroK8s is installed, you can check its status while it starts:</p> <pre><code>microk8s status --wait-ready\n</code></pre> <p>The Kubernetes CLI, <code>kubectl</code>, is available as a <code>microk8s</code> subcommand:</p> <pre><code>microk8s kubectl -h\n</code></pre> <p>For convenience, consider aliasing <code>microk8s kubectl</code>!</p>"},{"location":"setup/#ansible","title":"Ansible","text":"<p>Per its own documentation,</p> <p>Ansible is an IT automation tool. It can configure systems, deploy software, and orchestrate more advanced IT tasks such as continuous deployments or zero downtime rolling updates.</p> <p>To configure and deploy software to the cluster's Raspberry Pi worker nodes, we'll use Ansible.  Installing Ansible (on what will be the control node) requires a Python 3.8 or higher on the system, with pip installed as well. To ensure pip is installed on Ubuntu 22.04 LTS, for example, run:</p> <pre><code>sudo apt install python3-pip python3-pip-whl\n</code></pre> <p>Once done, Ansible can be user installed by running:</p> <pre><code>pip3 install --user ansible\n</code></pre>"},{"location":"topics/","title":"Learning topics","text":"<p>McCl8s was conceived with the idea of \"learning K8s\", but that idea is a fuzzy umbrella: it's not clear exactly what it covers. At a high level, the McCl8s project is meant to help learn:</p> <ol> <li>The basics of IT automation. K8s itself needs to be installed, and    its control and worker nodes configured to support deployment.</li> <li>K8s concepts. Understanding what K8s is for, its conceptual structure,    its design and its components, is prerequisite to understanding how to    use it.</li> <li>K8s management. Learning how to use K8s means learning how to manage    K8s application, resource, and policy objects.</li> <li>K8s-native tools and applications. There are K8s-native applications    to simplify and scale software development and ML lifecycles, including    CI/CD, workflow pipelines, model training, and application and model    serving and monitoring.</li> </ol>"},{"location":"vanilla/","title":"Vanilla Kubernetes","text":"<p>This page is under construction!</p> <p>Things to be captured here:</p> <ol> <li>The <code>kubelet</code> running on worker nodes expects <code>systemd-resolved</code> to be    used, whereas Raspberry Pi OS uses <code>dhcpcd</code> instead. As with Rocky Linux    users in this    kubeadm issue,    Raspberry Pi OS required a symbolic link from <code>/etc/resolv.conf</code> to    <code>/run/systemd/resolve</code>.</li> <li>The CNI plugin seems to be a requirement. Nodes can be connected as    workers, but some pods seem to require Calico or another CNI plugin to    be running.</li> <li>Calico needs to be created <code>kubectl -f create</code> not <code>apply</code>, because the    annotations for one of its CRDs exceeeds the allowed bytes length. So    unfortunately Kustomize can't be used.</li> <li>If running with swap disabled on WSL 2, beware the <code>swapoff -a</code> is a    session-only change, not a permanent one, and that there is no    <code>/etc/fstab</code> to modify. Instead, use the    <code>.wslconfig</code> swap setting    to disable swap for WSL 2 VMs.</li> </ol>"}]}